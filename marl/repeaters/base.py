from ..policy_output import PolicyOutput
import numpy as np
import torch
from loguru import logger
from marl.model_server.base_model_server import BaseModelServer
import time

def find_mask_begin(padded_datas, mask_id=0):
    """
    finding the mask id begin index and it's length
    """
    begin_indexs = []
    lengths = []

    for padded_data in padded_datas:
        is_flag = 0
        for index, data in enumerate(padded_data):
            if data != mask_id:
                is_flag = 1
                begin_indexs.append(index)
                length = (np.array(padded_data)!=mask_id).sum()
                lengths.append(length)
                break
        assert is_flag
    return begin_indexs, lengths

class RunningStates:
    # adopt from https://github.com/DLR-RM/stable-baselines3/blob/master/stable_baselines3/common/running_mean_std.py
    def __init__(self, epsilon: float = 1e-4):
        self.mean = torch.tensor(0, dtype=torch.float32)
        self.var = torch.tensor(0, dtype=torch.float32)
        self.count = epsilon

    def update(self, x: torch.Tensor):
        # TODO: adapt to dp
        x_var, x_mean = torch.var_mean(x.cpu(), unbiased=False)
        x_count = x.shape[0]
        self.update_from_moments(x_mean, x_var, x_count)

    def update_from_other(self, other: "RunningStates"):
        self.update_from_moments(other.mean, other.var, other.count)

    def update_from_moments(self, mean: torch.Tensor, var: torch.Tensor, count: int):
        delta = mean - self.mean
        tot_count = self.count + count
        m_a = self.var * self.count
        m_b = var * count
        m_2 = m_a + m_b + delta**2 * self.count * count / (self.count + count)
        new_var = m_2 / (self.count + count)

        self.mean += delta * count / tot_count
        self.var = new_var
        self.count = tot_count

    def state_dict(self):
        return dict(mean=self.mean, var=self.var, count=self.count)

    def load_state_dict(self, states):
        self.mean = states["mean"]
        self.var = states["var"]
        self.count = states["count"]

class BaseRepeater(object):
    def __init__(
            self, 
            sft_model, 
            reward_scale: bool = False,
            fine_grained_rm: bool = False,
            value_ema: bool = False,
            actor_micro_bs:int=8,
            ref_micro_bs:int=8,
            critic_micro_bs:int=32,
            kl_coeff = 0.02,
            gamma = 1.0,
            gae_lambda = 0.95,
            answer_end_id = 92542,
            norm_adv = False,
            norm_rewards = True,
            **_ignored,
        ):
        self.sft_model = sft_model
        self.actor_micro_bs = actor_micro_bs
        self.ref_micro_bs = ref_micro_bs
        self.critic_micro_bs = critic_micro_bs
        self.reward_scale = reward_scale
        self.fine_grained_rm = fine_grained_rm
        self.value_ema = value_ema
        self.kl_coeff = kl_coeff
        self.gamma = gamma
        self.gae_lambda = gae_lambda
        self.answer_end_id = answer_end_id
        self.norm_rewards = norm_rewards
        if self.norm_rewards:
            self.running_states = RunningStates(epsilon=0)

    def process(
            self, 
            trajectories:PolicyOutput, 
            policy_model:BaseModelServer, 
            value_model:BaseModelServer, 
            sft_model:BaseModelServer=None,
            env=None,  # only used for async reward model.infer_get() in _get_kl_rewards
        ):
        value_output_ref = self._get_values_async(trajectories, value_model)

        action_mask = trajectories["action_mask"]
        num_actions = action_mask.size(1)
        if sft_model is not None:
            self.sft_model:BaseModelServer = sft_model
        kl_rewards, entropy, kl_distance, policy_logprobs, sft_logprobs = self._get_kl_rewards(trajectories, policy_model,env=env)
        trajectories["kl"] = (kl_distance * action_mask).sum(axis=-1) / action_mask.sum(axis=-1)
        trajectories["entropy"] = entropy
        trajectories["kl_rewards"] = kl_rewards
        trajectories["policy_logprobs"] = policy_logprobs
        trajectories["sft_logprobs"] = sft_logprobs

        # values = self._get_values(trajectories, value_model)
        values = self._get_values_collect(value_output_ref, value_model)
        old_values = values[:, -num_actions:]
        advantages, returns = self.get_advantages_and_returns(old_values, kl_rewards, action_mask)
    
        trajectories["advantages"] = advantages
        trajectories["returns"] = returns
        trajectories["old_values"] = old_values

        return trajectories

    def _get_kl_rewards(self, trajectories: PolicyOutput, policy_model:BaseModelServer, env=None):
        s_t = time.time()
        policy_output = policy_model.infer_async(
            inputs=trajectories.output_ids, 
            micro_batch_size=self.actor_micro_bs, 
            attention_mask=trajectories.attention_mask, 
            output_logits=False, 
            output_logprobs=True
        )
        # policy_output = policy_model.infer_get(policy_output)
        # logger.info(f"[actor infer] duration: {round(time.time() - s_t, 2)} s")

        # s_t = time.time()
        sft_output = self.sft_model.infer_async(
            inputs=trajectories.output_ids, 
            micro_batch_size=self.ref_micro_bs, 
            attention_mask=trajectories.attention_mask, 
            output_logits=False, 
            output_logprobs=True
        )
        # sft_output = self.sft_model.infer_get(sft_output)
        # logger.info(f"[ref infer] duration: {round(time.time() - s_t, 2)} s")

        policy_output = policy_model.infer_get(policy_output)
        sft_output = self.sft_model.infer_get(sft_output)
        logger.info(f"[actor & ref infer_async] duration: {round(time.time() - s_t, 2)} s")

        ################## Experimental ################################################
        if env.async_reward:
            rewards = env.get_reward_collect(trajectories["reward_output_ref"])
            trajectories["reward_output_ref"] = None
            clipped_rewards = torch.clamp(rewards, min=env.clip_reward_min, max=env.clip_reward_max)
            trajectories["rewards"] = rewards
            trajectories["clipped_rewards"] = clipped_rewards
        ################## Experimental ################################################
        rewards = trajectories.clipped_rewards
        if self.norm_rewards:
            self.running_states.update(rewards)
            norm_reward_score = (rewards - self.running_states.mean) / (self.running_states.var.sqrt() + 1e-8)
        action_mask = trajectories.action_mask
        num_actions = action_mask.size(1)

        policy_logprobs = policy_output.logprobs[:, -num_actions:]
        sft_logprobs = sft_output.logprobs[:, -num_actions:]

        if self.kl_coeff <= 0.0:
            self.kl_coeff = 0.0
        # compute_approx_kl
        log_ratio = policy_logprobs - sft_logprobs
        kl = log_ratio * action_mask
        kl_reward = -self.kl_coeff * kl

        eos_indices = action_mask.size(1) - 1 - action_mask.long().fliplr().argmax(dim=1, keepdim=True)
        last_reward = torch.zeros_like(kl).scatter_(dim=1, index=eos_indices, src=norm_reward_score.unsqueeze(1).to(kl.dtype))

        reward = last_reward + kl_reward

        entropy = - (policy_logprobs * action_mask).sum(axis=-1) / action_mask.sum(axis=-1)
        return reward, entropy, kl, policy_logprobs, sft_logprobs

    def _get_values(self, trajectories: PolicyOutput, value_model:BaseModelServer):
        s_t = time.time()
        value_output = value_model.infer(
            inputs = trajectories.output_ids, 
            attention_mask=trajectories.attention_mask, 
            output_logits=True, 
            micro_batch_size=self.critic_micro_bs,
        )
        logger.info(f"[critic infer] duration: {round(time.time() - s_t, 2)} s")
        raw_values = value_output.logits.squeeze(-1)
        return raw_values

    def _get_values_async(self, trajectories: PolicyOutput, value_model:BaseModelServer):
        s_t = time.time()
        value_output_ref = value_model.infer_async(
            inputs = trajectories.output_ids, 
            attention_mask=trajectories.attention_mask, 
            output_logits=True, 
            micro_batch_size=self.critic_micro_bs,
        )
        logger.info(f"[critic infer] async duration: {round(time.time() - s_t, 2)} s")
        return value_output_ref

    def _get_values_collect(self, value_output_ref, value_model:BaseModelServer):
        s_t = time.time()
        value_output = value_model.infer_get(value_output_ref)
        raw_values = value_output.logits.squeeze(-1)
        logger.info(f"[critic infer] async wait duration: {round(time.time() - s_t, 2)} s")
        return raw_values

    def _get_advantages_and_returns(self, trajectories):
        output_ids = trajectories.output_ids
        answer_mask = trajectories.answer_mask
        values_with_last_value = trajectories.values_with_last_value
        kl_rewards = trajectories.kl_rewards

        begins_index, answers_length = find_mask_begin(answer_mask, 0)
        count = 0
        advantages_padded, returns_padded = torch.zeros_like(kl_rewards, dtype=values_with_last_value.dtype), torch.zeros_like(kl_rewards, dtype=values_with_last_value.dtype)
        for begin_index, ans_len, value_with_last_value, reward, output_id in zip(\
            begins_index, answers_length, values_with_last_value, kl_rewards, output_ids):
            # shape :ans_len + 1
            value_with_last_value = value_with_last_value[begin_index-1:begin_index+ans_len]
            # shape :ans_len
            reward = reward[begin_index:begin_index+ans_len]
            last_gae_lam = torch.zeros((1), dtype=values_with_last_value.dtype)
            # shape :ans_len
            advantages = torch.zeros_like(reward, dtype=values_with_last_value.dtype)
            step_nums = advantages.shape[-1]
            # shape:ans_len + 1
            dones = self._build_dones(output_id[begin_index:begin_index+ans_len])
            for step in reversed(range(step_nums)):
                next_non_terminal = 1 - dones[step + 1]
                next_values = value_with_last_value[step+1]
                # delta and last_gae_lam using value and reward 
                delta = reward[step] + self.gamma * next_values * next_non_terminal - value_with_last_value[step]
                last_gae_lam = delta + self.gamma * self.gae_lambda * next_non_terminal * last_gae_lam
                advantages[step] = last_gae_lam[0]
            returns = advantages + value_with_last_value[:-1]
            advantages_padded[count, begin_index:begin_index+ans_len] = advantages
            returns_padded[count, begin_index:begin_index+ans_len] = returns
            count += 1
        return advantages_padded, returns_padded

    # ans_len + 1: dones
    def _build_dones(self, answer_ids):
        dones = torch.tensor((answer_ids == self.answer_end_id).numpy().astype(np.float32))
        # (1, )the first one is not done, so obs_0_dones=0
        obs_0_dones = torch.zeros((1), dtype=torch.float32)
        # (ans_len + 1)，
        dones = torch.concat((obs_0_dones, dones), axis=0)
        return dones

    # TODO
    def _get_fine_grained_values(self, trajectories):
        pass

    def get_advantages_and_returns(
        self,
        values: torch.Tensor,
        rewards: torch.Tensor,
        action_mask: torch.Tensor,
    ):
        lastgaelam = 0
        advantages_reversed = []
        response_length = rewards.size(1)

        # Mask invalid responses
        values = action_mask * values
        rewards = action_mask * rewards

        for t in reversed(range(response_length)):
            nextvalues = values[:, t + 1] if t < response_length - 1 else 0.0
            delta = rewards[:, t] + self.gamma * nextvalues - values[:, t]
            lastgaelam = delta + self.gamma * self.gae_lambda * lastgaelam
            advantages_reversed.append(lastgaelam)
        advantages = torch.stack(advantages_reversed[::-1], dim=1)
        returns = advantages + values
        return advantages.detach(), returns